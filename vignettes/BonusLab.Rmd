---
title: "BonusLab"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{BonusLab}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(BonusLab)
```
#============= RUNNING THE MODEL ==================
# THATS CROSS VALIDIATION(1.2.5)

ridge_model <- ridgereg_model()
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 5)
rGrid <- expand.grid(lambda = 1*10**(c(1:10)))
set.seed(825) # Maybe remove it
rrTune <- train(form = medv ~ .,
                data = training_set,
                method = ridge_model,
                trControl = fitControl,
                tuneGrid = rGrid)


print(rrTune)
plot(rrTune)



#===== REDURANT CODE ==============


library(ggplot2)
ggplot(rrTune$results) +
  geom_point(aes(x= log10(lambda), y = RMSE, colour="#2c3e50", size=2)) +
  geom_line(aes(x= log10(lambda), y = RMSE, colour="#2980b9", size=1)) +
  theme(legend.position = "none") +
  scale_x_continuous(breaks=seq(0,24,2))


### 3.6. Evaluate performance of all three models on test dataset.
As our outcome is numeric, we will use the function `postResample` in order to estimate the root mean squared error (RMSE), the coefficient of determination ($R^2$), and the mean absolute error (MAE).
```{r}
pred <- predict(linregFit, testing)
postResample(pred = pred, obs = testing$medv)
```

And the same for the forward looking model.
```{r}
forpred <- predict(linforFit, testing)
postResample(pred = forpred, obs = testing$medv)
```

```{r}
my_ridge <- ridgereg$new(medv~., data=training, lambda = 10**6)
X_test <- dplyr::select(testing, -medv)
ridge_pred <- my_ridge$predict(X_test)
postResample(pred = ridge_pred, obs = testing$medv)
```